{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXtAHSmnDKs_"
      },
      "source": [
        "Questão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YRkdtvKDKtA"
      },
      "source": [
        "Carregando dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fKJqq_yYDKtB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar dados de treinamento\n",
        "train_data = pd.read_csv('credtrain.txt', sep='\\t', header=None)\n",
        "test_data = pd.read_csv('credtest.txt', sep='\\t', header=None)\n",
        "\n",
        "# Definir os nomes das colunas\n",
        "columns = [\n",
        "    'ESCT', 'NDEP', 'RENDA', 'TIPOR', 'VBEM', 'NPARC', 'VPARC', 'TEL', 'IDADE', 'RESMS', 'ENTRADA', 'CLASSE'\n",
        "]\n",
        "train_data.columns = columns\n",
        "test_data.columns = columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxQGF8ZVDKtB"
      },
      "source": [
        "Passo 2: Pré-processamento dos dados\n",
        "Precisamos codificar os atributos categóricos e normalizar os atributos contínuos.\n",
        "\n",
        "Codificação de atributos categóricos\n",
        "Vamos usar One-Hot Encoding para os atributos categóricos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PzgKFHYfDKtC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Selecionar colunas categóricas\n",
        "categorical_cols = ['ESCT', 'NDEP', 'TIPOR', 'TEL']\n",
        "\n",
        "# Aplicar One-Hot Encoding\n",
        "# The 'sparse' argument has been replaced with 'sparse_output'\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "# Criar DataFrames com as colunas codificadas\n",
        "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Remover colunas categóricas originais e adicionar as codificadas\n",
        "train_data = train_data.drop(categorical_cols, axis=1).join(encoded_train_df)\n",
        "test_data = test_data.drop(categorical_cols, axis=1).join(encoded_test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOkpXL4DKtC"
      },
      "source": [
        "Normalização de atributos contínuos\n",
        "Vamos usar StandardScaler para normalizar os atributos contínuos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "fW7nAoY0DKtD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selecionar colunas numéricas\n",
        "numeric_cols = ['RENDA', 'VBEM', 'NPARC', 'VPARC', 'IDADE', 'RESMS', 'ENTRADA']\n",
        "\n",
        "# Aplicar StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data[numeric_cols] = scaler.fit_transform(train_data[numeric_cols])\n",
        "test_data[numeric_cols] = scaler.transform(test_data[numeric_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYvGDOknDKtD"
      },
      "source": [
        "Passo 3: Separar features e target\n",
        "Agora, vamos separar as features (X) e o target (y) para treinamento e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9gE-5LRrDKtD"
      },
      "outputs": [],
      "source": [
        "# Separar features e target\n",
        "X_train = train_data.drop('CLASSE', axis=1)\n",
        "y_train = train_data['CLASSE']\n",
        "X_test = test_data.drop('CLASSE', axis=1)\n",
        "y_test = test_data['CLASSE']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaREk49gDKtE"
      },
      "source": [
        "Passo 4: Treinar os modelos\n",
        "Vamos treinar os modelos LogisticRegression, KNeighborsClassifier e XGBClassifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "AqLsL3EyDKtE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Modelo 1: Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Modelo 2: K-Nearest Neighbors\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Modelo 3: XGBoost\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJuDk04nDKtE"
      },
      "source": [
        "Passo 5: Avaliar os modelos\n",
        "Vamos avaliar os modelos usando a matriz de confusão e o relatório de classificação.\n",
        "\n",
        "Matriz de Confusão e Relatório de Classificação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLmHRwdxDKtE",
        "outputId": "22880f72-d7b9-4ad2-8266-4ff05c67d639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusão - Logistic Regression:\n",
            "[[291  15]\n",
            " [ 48 223]]\n",
            "\n",
            "Relatório de Classificação - Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.95      0.90       306\n",
            "           1       0.94      0.82      0.88       271\n",
            "\n",
            "    accuracy                           0.89       577\n",
            "   macro avg       0.90      0.89      0.89       577\n",
            "weighted avg       0.90      0.89      0.89       577\n",
            "\n",
            "Matriz de Confusão - K-Nearest Neighbors:\n",
            "[[275  31]\n",
            " [ 53 218]]\n",
            "\n",
            "Relatório de Classificação - K-Nearest Neighbors:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       306\n",
            "           1       0.88      0.80      0.84       271\n",
            "\n",
            "    accuracy                           0.85       577\n",
            "   macro avg       0.86      0.85      0.85       577\n",
            "weighted avg       0.86      0.85      0.85       577\n",
            "\n",
            "Matriz de Confusão - XGBoost:\n",
            "[[273  33]\n",
            " [ 40 231]]\n",
            "\n",
            "Relatório de Classificação - XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88       306\n",
            "           1       0.88      0.85      0.86       271\n",
            "\n",
            "    accuracy                           0.87       577\n",
            "   macro avg       0.87      0.87      0.87       577\n",
            "weighted avg       0.87      0.87      0.87       577\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Função para exibir resultados\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"Matriz de Confusão - {model_name}:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(f\"\\nRelatório de Classificação - {model_name}:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Avaliar Logistic Regression\n",
        "evaluate_model(y_test, y_pred_log_reg, \"Logistic Regression\")\n",
        "\n",
        "# Avaliar K-Nearest Neighbors\n",
        "evaluate_model(y_test, y_pred_knn, \"K-Nearest Neighbors\")\n",
        "\n",
        "# Avaliar XGBoost\n",
        "evaluate_model(y_test, y_pred_xgb, \"XGBoost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXtz9TwDKtE"
      },
      "source": [
        "Resultados Esperados\n",
        "A matriz de confusão mostrará os valores de verdadeiros positivos, falsos positivos, verdadeiros negativos e falsos negativos.\n",
        "\n",
        "O relatório de classificação mostrará métricas como precisão, recall, F1-score e acurácia para cada modelo.\n",
        "\n",
        "Conclusão\n",
        "Com isso, você terá os modelos treinados e avaliados"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
