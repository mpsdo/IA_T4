{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXtAHSmnDKs_"
      },
      "source": [
        "## Questão 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YRkdtvKDKtA"
      },
      "source": [
        "Carregando dados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fKJqq_yYDKtB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar dados de treinamento\n",
        "train_data = pd.read_csv('credtrain.txt', sep='\\t', header=None)\n",
        "test_data = pd.read_csv('credtest.txt', sep='\\t', header=None)\n",
        "\n",
        "# Definir os nomes das colunas\n",
        "columns = [\n",
        "    'ESCT', 'NDEP', 'RENDA', 'TIPOR', 'VBEM', 'NPARC', 'VPARC', 'TEL', 'IDADE', 'RESMS', 'ENTRADA', 'CLASSE'\n",
        "]\n",
        "train_data.columns = columns\n",
        "test_data.columns = columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxQGF8ZVDKtB"
      },
      "source": [
        "Passo 2: Pré-processamento dos dados\n",
        "Precisamos codificar os atributos categóricos e normalizar os atributos contínuos.\n",
        "\n",
        "Codificação de atributos categóricos\n",
        "Vamos usar One-Hot Encoding para os atributos categóricos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PzgKFHYfDKtC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Selecionar colunas categóricas\n",
        "categorical_cols = ['ESCT', 'NDEP', 'TIPOR', 'TEL']\n",
        "\n",
        "# Aplicar One-Hot Encoding\n",
        "# The 'sparse' argument has been replaced with 'sparse_output'\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
        "encoded_train = encoder.fit_transform(train_data[categorical_cols])\n",
        "encoded_test = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "# Criar DataFrames com as colunas codificadas\n",
        "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Remover colunas categóricas originais e adicionar as codificadas\n",
        "train_data = train_data.drop(categorical_cols, axis=1).join(encoded_train_df)\n",
        "test_data = test_data.drop(categorical_cols, axis=1).join(encoded_test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWOkpXL4DKtC"
      },
      "source": [
        "Normalização de atributos contínuos\n",
        "Vamos usar StandardScaler para normalizar os atributos contínuos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fW7nAoY0DKtD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selecionar colunas numéricas\n",
        "numeric_cols = ['RENDA', 'VBEM', 'NPARC', 'VPARC', 'IDADE', 'RESMS', 'ENTRADA']\n",
        "\n",
        "# Aplicar StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data[numeric_cols] = scaler.fit_transform(train_data[numeric_cols])\n",
        "test_data[numeric_cols] = scaler.transform(test_data[numeric_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYvGDOknDKtD"
      },
      "source": [
        "Passo 3: Separar features e target\n",
        "Agora, vamos separar as features (X) e o target (y) para treinamento e teste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9gE-5LRrDKtD"
      },
      "outputs": [],
      "source": [
        "# Separar features e target\n",
        "X_train = train_data.drop('CLASSE', axis=1)\n",
        "y_train = train_data['CLASSE']\n",
        "X_test = test_data.drop('CLASSE', axis=1)\n",
        "y_test = test_data['CLASSE']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaREk49gDKtE"
      },
      "source": [
        "Passo 4: Treinar os modelos\n",
        "Vamos treinar os modelos LogisticRegression, KNeighborsClassifier e XGBClassifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AqLsL3EyDKtE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Modelo 1: Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Modelo 2: K-Nearest Neighbors\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Modelo 3: XGBoost\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJuDk04nDKtE"
      },
      "source": [
        "Passo 5: Avaliar os modelos\n",
        "Vamos avaliar os modelos usando a matriz de confusão e o relatório de classificação.\n",
        "\n",
        "Matriz de Confusão e Relatório de Classificação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLmHRwdxDKtE",
        "outputId": "22880f72-d7b9-4ad2-8266-4ff05c67d639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusão - Logistic Regression:\n",
            "[[291  15]\n",
            " [ 48 223]]\n",
            "\n",
            "Relatório de Classificação - Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.95      0.90       306\n",
            "           1       0.94      0.82      0.88       271\n",
            "\n",
            "    accuracy                           0.89       577\n",
            "   macro avg       0.90      0.89      0.89       577\n",
            "weighted avg       0.90      0.89      0.89       577\n",
            "\n",
            "Matriz de Confusão - K-Nearest Neighbors:\n",
            "[[275  31]\n",
            " [ 53 218]]\n",
            "\n",
            "Relatório de Classificação - K-Nearest Neighbors:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.90      0.87       306\n",
            "           1       0.88      0.80      0.84       271\n",
            "\n",
            "    accuracy                           0.85       577\n",
            "   macro avg       0.86      0.85      0.85       577\n",
            "weighted avg       0.86      0.85      0.85       577\n",
            "\n",
            "Matriz de Confusão - XGBoost:\n",
            "[[273  33]\n",
            " [ 40 231]]\n",
            "\n",
            "Relatório de Classificação - XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88       306\n",
            "           1       0.88      0.85      0.86       271\n",
            "\n",
            "    accuracy                           0.87       577\n",
            "   macro avg       0.87      0.87      0.87       577\n",
            "weighted avg       0.87      0.87      0.87       577\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Função para exibir resultados\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"Matriz de Confusão - {model_name}:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(f\"\\nRelatório de Classificação - {model_name}:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Avaliar Logistic Regression\n",
        "evaluate_model(y_test, y_pred_log_reg, \"Logistic Regression\")\n",
        "\n",
        "# Avaliar K-Nearest Neighbors\n",
        "evaluate_model(y_test, y_pred_knn, \"K-Nearest Neighbors\")\n",
        "\n",
        "# Avaliar XGBoost\n",
        "evaluate_model(y_test, y_pred_xgb, \"XGBoost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXtz9TwDKtE"
      },
      "source": [
        "Resultados Esperados\n",
        "A matriz de confusão mostrará os valores de verdadeiros positivos, falsos positivos, verdadeiros negativos e falsos negativos.\n",
        "\n",
        "O relatório de classificação mostrará métricas como precisão, recall, F1-score e acurácia para cada modelo.\n",
        "\n",
        "Conclusão\n",
        "Com isso, você terá os modelos treinados e avaliados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Questão 2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passo 1: Carregar os Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carregar dados de treinamento e teste\n",
        "train_data = pd.read_csv('credtrain.txt', sep='\\t', header=None)\n",
        "test_data = pd.read_csv('credtest.txt', sep='\\t', header=None)\n",
        "\n",
        "# Definir nomes das colunas\n",
        "columns = ['ESCT', 'NDEP', 'RENDA', 'TIPOR', 'VBEM', 'NPARC', 'VPARC', 'TEL', 'IDADE', 'RESMS', 'ENTRADA', 'CLASSE']\n",
        "train_data.columns = columns\n",
        "test_data.columns = columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passo 2: Aplicar Target Encoding\n",
        "Vamos usar a biblioteca category_encoders para aplicar o Target Encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from category_encoders import TargetEncoder\n",
        "\n",
        "# Selecionar colunas categóricas\n",
        "categorical_cols = ['ESCT', 'NDEP', 'TIPOR', 'TEL']\n",
        "\n",
        "# Aplicar Target Encoding\n",
        "encoder = TargetEncoder(cols=categorical_cols)\n",
        "train_data_encoded = encoder.fit_transform(train_data[categorical_cols], train_data['CLASSE'])\n",
        "test_data_encoded = encoder.transform(test_data[categorical_cols])\n",
        "\n",
        "# Substituir colunas categóricas originais pelas codificadas\n",
        "train_data[categorical_cols] = train_data_encoded\n",
        "test_data[categorical_cols] = test_data_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passo 3: Normalizar Atributos Numéricos\n",
        "A normalização dos atributos numéricos já foi feita na Questão 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Selecionar colunas numéricas\n",
        "numeric_cols = ['RENDA', 'VBEM', 'NPARC', 'VPARC', 'IDADE', 'RESMS', 'ENTRADA']\n",
        "\n",
        "# Aplicar StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data[numeric_cols] = scaler.fit_transform(train_data[numeric_cols])\n",
        "test_data[numeric_cols] = scaler.transform(test_data[numeric_cols])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passo 4: Separar Features e Target\n",
        "Separe as features (X) e o target (y) para treinamento e teste:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separar features e target\n",
        "X_train = train_data.drop('CLASSE', axis=1)\n",
        "y_train = train_data['CLASSE']\n",
        "X_test = test_data.drop('CLASSE', axis=1)\n",
        "y_test = test_data['CLASSE']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passo 5: Treinar e Avaliar Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, treine os modelos (Logistic Regression, K-Nearest Neighbors e XGBoost) usando os dados codificados com Target Encoding e compare os resultados com os obtidos na Questão 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de Confusão - Logistic Regression:\n",
            "[[287  19]\n",
            " [ 49 222]]\n",
            "\n",
            "Relatório de Classificação - Logistic Regression:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.94      0.89       306\n",
            "           1       0.92      0.82      0.87       271\n",
            "\n",
            "    accuracy                           0.88       577\n",
            "   macro avg       0.89      0.88      0.88       577\n",
            "weighted avg       0.89      0.88      0.88       577\n",
            "\n",
            "Matriz de Confusão - K-Nearest Neighbors:\n",
            "[[263  43]\n",
            " [ 61 210]]\n",
            "\n",
            "Relatório de Classificação - K-Nearest Neighbors:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       306\n",
            "           1       0.83      0.77      0.80       271\n",
            "\n",
            "    accuracy                           0.82       577\n",
            "   macro avg       0.82      0.82      0.82       577\n",
            "weighted avg       0.82      0.82      0.82       577\n",
            "\n",
            "Matriz de Confusão - XGBoost:\n",
            "[[279  27]\n",
            " [ 42 229]]\n",
            "\n",
            "Relatório de Classificação - XGBoost:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.91      0.89       306\n",
            "           1       0.89      0.85      0.87       271\n",
            "\n",
            "    accuracy                           0.88       577\n",
            "   macro avg       0.88      0.88      0.88       577\n",
            "weighted avg       0.88      0.88      0.88       577\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Modelo 1: Logistic Regression\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "# Modelo 2: K-Nearest Neighbors\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Modelo 3: XGBoost\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "\n",
        "# Avaliar modelos\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"Matriz de Confusão - {model_name}:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(f\"\\nRelatório de Classificação - {model_name}:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "evaluate_model(y_test, y_pred_log_reg, \"Logistic Regression\")\n",
        "evaluate_model(y_test, y_pred_knn, \"K-Nearest Neighbors\")\n",
        "evaluate_model(y_test, y_pred_xgb, \"XGBoost\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Questão 3 (finalizar a questão, não consegui achar os arquivos pra load)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Dados Necessários\n",
        "A questão menciona que são fornecidos arquivos no formato .npy:\n",
        "\n",
        "X_train.npy: Dados de treinamento (features).\n",
        "\n",
        "y_train.npy: Labels de treinamento (variável alvo).\n",
        "\n",
        "X_val.npy: Dados de validação (features).\n",
        "\n",
        "y_val.npy: Labels de validação (variável alvo).\n",
        "\n",
        "X_test.npy: Dados de teste (features).\n",
        "\n",
        "y_test.npy: Labels de teste (variável alvo).\n",
        "\n",
        "Esses arquivos contêm os conjuntos de dados para treinamento, validação e teste, já pré-processados. Se você não tiver esses arquivos, será necessário obtê-los ou gerá-los."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Passos para Implementar a Questão 3\n",
        "\n",
        "Passo 1: Carregar os Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'X_train.npy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Carregar dados\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_train.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_train.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m X_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_val.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\marco\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'X_train.npy'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Carregar dados\n",
        "X_train = np.load('X_train.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "X_val = np.load('X_val.npy')\n",
        "y_val = np.load('y_val.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_test = np.load('y_test.npy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Passo 2: Verificar Desbalanceamento\n",
        "Verifique se os dados estão desbalanceados, ou seja, se uma classe tem muito mais exemplos que a outra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'y_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Verificar balanceamento das classes\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(\u001b[43my_train\u001b[49m, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribuição das classes no treino:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(unique, counts)))\n\u001b[0;32m      7\u001b[0m unique, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_test, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Verificar balanceamento das classes\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Distribuição das classes no treino:\", dict(zip(unique, counts)))\n",
        "\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "print(\"Distribuição das classes no teste:\", dict(zip(unique, counts)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se uma classe tiver muito mais exemplos que a outra, os dados estão desbalanceados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Técnicas para Lidar com Conjuntos Desbalanceados\n",
        "A questão pede para testar três técnicas de balanceamento:\n",
        "\n",
        "Undersampling: Reduzir a quantidade de exemplos da classe majoritária.\n",
        "\n",
        "Oversampling: Aumentar a quantidade de exemplos da classe minoritária.\n",
        "\n",
        "Alteração de Limiar: Ajustar o limiar de decisão do modelo para melhorar a classificação da classe minoritária.\n",
        "\n",
        "Técnica 1: Undersampling\n",
        "Reduza a quantidade de exemplos da classe majoritária para equilibrar as classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Aplicar undersampling\n",
        "undersampler = RandomUnderSampler(random_state=42)\n",
        "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Verificar balanceamento após undersampling\n",
        "unique, counts = np.unique(y_train_under, return_counts=True)\n",
        "print(\"Distribuição após undersampling:\", dict(zip(unique, counts)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Técnica 2: Oversampling\n",
        "Aumente a quantidade de exemplos da classe minoritária usando técnicas como SMOTE (Synthetic Minority Over-sampling Technique)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Aplicar oversampling com SMOTE\n",
        "oversampler = SMOTE(random_state=42)\n",
        "X_train_over, y_train_over = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "# Verificar balanceamento após oversampling\n",
        "unique, counts = np.unique(y_train_over, return_counts=True)\n",
        "print(\"Distribuição após oversampling:\", dict(zip(unique, counts)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Técnica 3: Alteração de Limiar\n",
        "Ajuste o limiar de decisão do modelo para melhorar a classificação da classe minoritária."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Treinar modelo sem ajuste de limiar\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Prever probabilidades\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Ajustar limiar (exemplo: 0.3)\n",
        "threshold = 0.3\n",
        "y_pred_adjusted = (y_pred_proba > threshold).astype(int)\n",
        "\n",
        "# Avaliar modelo com limiar ajustado\n",
        "print(\"Matriz de Confusão com Limiar Ajustado:\")\n",
        "print(confusion_matrix(y_test, y_pred_adjusted))\n",
        "print(\"\\nRelatório de Classificação com Limiar Ajustado:\")\n",
        "print(classification_report(y_test, y_pred_adjusted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Avaliação dos Modelos\n",
        "Avalie os modelos treinados com cada técnica usando métricas como precisão, recall, F1-score e matriz de confusão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Função para avaliar modelos\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"Matriz de Confusão - {model_name}:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(f\"\\nRelatório de Classificação - {model_name}:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Exemplo de avaliação para undersampling\n",
        "model_under = LogisticRegression()\n",
        "model_under.fit(X_train_under, y_train_under)\n",
        "y_pred_under = model_under.predict(X_test)\n",
        "evaluate_model(y_test, y_pred_under, \"Undersampling\")\n",
        "\n",
        "# Exemplo de avaliação para oversampling\n",
        "model_over = LogisticRegression()\n",
        "model_over.fit(X_train_over, y_train_over)\n",
        "y_pred_over = model_over.predict(X_test)\n",
        "evaluate_model(y_test, y_pred_over, \"Oversampling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Comparação das Técnicas\n",
        "Compare os resultados das três técnicas (undersampling, oversampling e alteração de limiar) para determinar qual é mais eficaz no seu problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "6. Bibliotecas Necessárias\n",
        "Certifique-se de ter as seguintes bibliotecas instaladas:\n",
        "\n",
        "numpy\n",
        "\n",
        "scikit-learn\n",
        "\n",
        "imblearn (para técnicas de balanceamento)\n",
        "\n",
        "Se não tiver a biblioteca imblearn, instale-a com:\n",
        "\n",
        "pip install imbalanced-learn"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
